{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da9dd5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# notebooks/01_text_prototype_improved.ipynb\n",
    "\n",
    "# ==============================\n",
    "# Phase 2 - Enhanced Text-only Prototype\n",
    "# ==============================\n",
    "\n",
    "# --- Imports ---\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# =================================\n",
    "# 1. Load and preprocess dataset\n",
    "# =================================\n",
    "def load_and_split_data(data_path: str) -> tuple:\n",
    "    \"\"\"Load Pima dataset and split into train/test\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "        \n",
    "        X = df.drop(\"Outcome\", axis=1)\n",
    "        y = df[\"Outcome\"]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        print(f\"Positive class ratio - Train: {y_train.mean():.2f}, Test: {y_test.mean():.2f}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load data\n",
    "data_path = \"../data/raw/diabetes.csv\"\n",
    "X_train, X_test, y_train, y_test = load_and_split_data(data_path)\n",
    "\n",
    "# =================================\n",
    "# 2. Train and evaluate baseline classifiers\n",
    "# =================================\n",
    "def train_classifiers(X_train, y_train, X_test, y_test) -> tuple:\n",
    "    \"\"\"Train and evaluate baseline classifiers\"\"\"\n",
    "    \n",
    "    # Logistic Regression\n",
    "    print(\"\\n=== Training Logistic Regression ===\")\n",
    "    logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    logreg_pred = logreg.predict(X_test)\n",
    "    print(\"Logistic Regression Report:\")\n",
    "    print(classification_report(y_test, logreg_pred))\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"\\n=== Training XGBoost ===\")\n",
    "    xgb = XGBClassifier(\n",
    "        use_label_encoder=False, \n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    xgb.fit(X_train, y_train)\n",
    "    \n",
    "    xgb_pred = xgb.predict(X_test)\n",
    "    print(\"XGBoost Report:\")\n",
    "    print(classification_report(y_test, xgb_pred))\n",
    "    \n",
    "    # Save models\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "    joblib.dump(logreg, \"../models/logreg.pkl\")\n",
    "    joblib.dump(xgb, \"../models/xgb.pkl\")\n",
    "    print(\"\\nModels saved to ../models/\")\n",
    "    \n",
    "    return logreg, xgb\n",
    "\n",
    "# Train models\n",
    "logreg, xgb = train_classifiers(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# =================================\n",
    "# 3. Enhanced FAISS RAG index with metadata\n",
    "# =================================\n",
    "def build_knowledge_base() -> tuple:\n",
    "    \"\"\"Build enhanced knowledge base with more comprehensive diabetes information\"\"\"\n",
    "    \n",
    "    # Enhanced document collection with titles for better sourcing\n",
    "    knowledge_docs = [\n",
    "        {\n",
    "            \"title\": \"WHO Diabetes Definition\",\n",
    "            \"content\": \"Diabetes is a chronic disease characterized by elevated blood glucose levels. When the body cannot produce enough insulin or cannot effectively use the insulin it produces, glucose builds up in the bloodstream.\",\n",
    "            \"source\": \"WHO Guidelines\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Lifestyle Interventions\",\n",
    "            \"content\": \"Lifestyle interventions such as diet and exercise are highly effective in managing diabetes. A combination of regular physical activity and healthy eating can significantly reduce diabetes risk and improve glycemic control.\",\n",
    "            \"source\": \"IDF Clinical Practice Guidelines\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"WHO Screening Recommendations\",\n",
    "            \"content\": \"The WHO recommends screening for diabetes in adults with risk factors such as obesity, family history, age over 45, and sedentary lifestyle. Early detection enables timely intervention and better outcomes.\",\n",
    "            \"source\": \"WHO Diabetes Screening Guidelines\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Global Diabetes Statistics\",\n",
    "            \"content\": \"The IDF estimates that over 537 million adults worldwide are living with diabetes, with projections reaching 783 million by 2045. The majority have type 2 diabetes, which is largely preventable.\",\n",
    "            \"source\": \"IDF Diabetes Atlas 10th Edition\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Diabetes Complications\",\n",
    "            \"content\": \"Uncontrolled diabetes can lead to serious complications including cardiovascular disease, kidney disease, diabetic retinopathy leading to blindness, and diabetic neuropathy. These complications are largely preventable with proper management.\",\n",
    "            \"source\": \"WHO Diabetes Complications Report\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Risk Factors Assessment\",\n",
    "            \"content\": \"Key diabetes risk factors include high BMI, glucose intolerance, insulin resistance, family history, age, ethnicity, and gestational diabetes history. Multiple risk factors compound the overall diabetes risk.\",\n",
    "            \"source\": \"ADA Risk Assessment Guidelines\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Diagnostic Criteria\",\n",
    "            \"content\": \"Diabetes diagnosis is confirmed by fasting plasma glucose ≥126 mg/dL, random plasma glucose ≥200 mg/dL with symptoms, or HbA1c ≥6.5%. Oral glucose tolerance test showing 2-hour glucose ≥200 mg/dL also confirms diagnosis.\",\n",
    "            \"source\": \"WHO Diagnostic Criteria\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Extract content and metadata\n",
    "    docs_content = [doc[\"content\"] for doc in knowledge_docs]\n",
    "    docs_metadata = [{k: v for k, v in doc.items() if k != \"content\"} for doc in knowledge_docs]\n",
    "    \n",
    "    # Load embedding model\n",
    "    print(\"Loading embedding model...\")\n",
    "    embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Creating document embeddings...\")\n",
    "    doc_embeddings = embedder.encode(docs_content, convert_to_numpy=True)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    dim = doc_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(doc_embeddings)\n",
    "    \n",
    "    print(f\"FAISS index built with {len(docs_content)} documents, dimension: {dim}\")\n",
    "    \n",
    "    return embedder, index, docs_content, docs_metadata\n",
    "\n",
    "# Build knowledge base\n",
    "embedder, index, docs_content, docs_metadata = build_knowledge_base()\n",
    "\n",
    "# =================================\n",
    "# 4. Enhanced RAG + LLM pipeline\n",
    "# =================================\n",
    "def retrieve_relevant_docs(query: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve relevant documents with metadata\"\"\"\n",
    "    try:\n",
    "        query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "        distances, indices = index.search(query_vec, k)\n",
    "        \n",
    "        retrieved_docs = []\n",
    "        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):\n",
    "            retrieved_docs.append({\n",
    "                \"rank\": i + 1,\n",
    "                \"content\": docs_content[idx],\n",
    "                \"title\": docs_metadata[idx][\"title\"],\n",
    "                \"source\": docs_metadata[idx][\"source\"],\n",
    "                \"similarity_score\": float(1 / (1 + dist))  # Convert distance to similarity\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "def format_risk_level(probability: float) -> str:\n",
    "    \"\"\"Convert probability to risk level description\"\"\"\n",
    "    if probability >= 0.7:\n",
    "        return \"High Risk\"\n",
    "    elif probability >= 0.4:\n",
    "        return \"Moderate Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "def create_enhanced_prompt(patient_features: List[float], probability: float, retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"Create enhanced prompt for LLM with better structure\"\"\"\n",
    "    \n",
    "    # Feature names for context\n",
    "    feature_names = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \n",
    "                    \"Insulin\", \"BMI\", \"DiabetesPedigreeFunction\", \"Age\"]\n",
    "    \n",
    "    # Format patient data\n",
    "    patient_data = \", \".join([f\"{name}: {val}\" for name, val in zip(feature_names, patient_features)])\n",
    "    risk_level = format_risk_level(probability)\n",
    "    \n",
    "    # Format retrieved documents\n",
    "    context_docs = \"\\n\".join([\n",
    "        f\"- {doc['title']}: {doc['content']} (Source: {doc['source']})\"\n",
    "        for doc in retrieved_docs\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are a medical AI assistant providing diabetes risk assessment explanations.\n",
    "\n",
    "PATIENT DATA: {patient_data}\n",
    "PREDICTED DIABETES RISK: {probability:.1%} ({risk_level})\n",
    "\n",
    "RELEVANT MEDICAL GUIDELINES:\n",
    "{context_docs}\n",
    "\n",
    "Please provide a JSON response with exactly these fields:\n",
    "- \"conclusion\": A clear summary of the diabetes risk assessment\n",
    "- \"reasoning\": Detailed explanation of why this risk level was determined, referencing specific patient factors and guidelines\n",
    "- \"sources\": List of guideline sources that support this assessment\n",
    "\n",
    "Ensure the response is valid JSON format.\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def extract_json_from_response(response_text: str) -> Dict:\n",
    "    \"\"\"Extract and validate JSON from LLM response\"\"\"\n",
    "    try:\n",
    "        # Look for JSON-like content\n",
    "        json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            return json.loads(json_str)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: create structured response\n",
    "    return {\n",
    "        \"conclusion\": f\"Unable to parse structured response from LLM\",\n",
    "        \"reasoning\": response_text.strip(),\n",
    "        \"sources\": [\"Response parsing error\"]\n",
    "    }\n",
    "\n",
    "def enhanced_rag_pipeline(patient_features: List[float], model_name: str = \"xgb\") -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced RAG pipeline with better error handling and output format\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Get classifier prediction\n",
    "        model = xgb if model_name == \"xgb\" else logreg\n",
    "        input_data = np.array(patient_features).reshape(1, -1)\n",
    "        probability = float(model.predict_proba(input_data)[0, 1])\n",
    "        prediction = int(probability > 0.5)\n",
    "        \n",
    "        # 2. Retrieve relevant documents\n",
    "        query = f\"diabetes risk factors prediction probability {probability:.1%}\"\n",
    "        retrieved_docs = retrieve_relevant_docs(query, k=3)\n",
    "        \n",
    "        # 3. Generate LLM response\n",
    "        prompt = create_enhanced_prompt(patient_features, probability, retrieved_docs)\n",
    "        \n",
    "        # Initialize LLM (you might want to replace with OpenAI API for better results)\n",
    "        llm = pipeline(\"text-generation\", \n",
    "                      model=\"microsoft/DialoGPT-medium\",\n",
    "                      max_length=512,\n",
    "                      do_sample=True,\n",
    "                      temperature=0.7)\n",
    "        \n",
    "        response = llm(prompt, max_new_tokens=200, num_return_sequences=1)[0][\"generated_text\"]\n",
    "        \n",
    "        # Extract the response part (remove the prompt)\n",
    "        llm_response = response[len(prompt):].strip()\n",
    "        \n",
    "        # 4. Parse LLM response\n",
    "        structured_response = extract_json_from_response(llm_response)\n",
    "        \n",
    "        # 5. Compile final result\n",
    "        result = {\n",
    "            \"model_used\": model_name,\n",
    "            \"prediction\": prediction,\n",
    "            \"probability\": probability,\n",
    "            \"risk_level\": format_risk_level(probability),\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"llm_explanation\": structured_response,\n",
    "            \"raw_llm_response\": llm_response\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Pipeline error: {str(e)}\",\n",
    "            \"model_used\": model_name,\n",
    "            \"prediction\": None,\n",
    "            \"probability\": None\n",
    "        }\n",
    "\n",
    "# =================================\n",
    "# 5. Test enhanced pipeline\n",
    "# =================================\n",
    "def test_pipeline():\n",
    "    \"\"\"Test the enhanced pipeline with multiple samples\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING ENHANCED RAG PIPELINE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test with different patient profiles\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"High Risk Patient\",\n",
    "            \"features\": X_test.iloc[0].tolist(),\n",
    "            \"model\": \"xgb\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Low Risk Patient\", \n",
    "            \"features\": [1, 85, 66, 29, 0, 26.6, 0.351, 31],  # Manually created low-risk profile\n",
    "            \"model\": \"logreg\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- Test Case {i}: {test_case['name']} ---\")\n",
    "        \n",
    "        result = enhanced_rag_pipeline(test_case['features'], test_case['model'])\n",
    "        \n",
    "        if 'error' in result:\n",
    "            print(f\"ERROR: {result['error']}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Model: {result['model_used']}\")\n",
    "        print(f\"Prediction: {result['prediction']} (Risk: {result['risk_level']})\")\n",
    "        print(f\"Probability: {result['probability']:.1%}\")\n",
    "        \n",
    "        print(f\"\\nRetrieved Documents:\")\n",
    "        for doc in result['retrieved_documents']:\n",
    "            print(f\"  {doc['rank']}. {doc['title']} (Score: {doc['similarity_score']:.3f})\")\n",
    "        \n",
    "        print(f\"\\nLLM Explanation:\")\n",
    "        if isinstance(result['llm_explanation'], dict):\n",
    "            for key, value in result['llm_explanation'].items():\n",
    "                print(f\"  {key.title()}: {value}\")\n",
    "        else:\n",
    "            print(f\"  {result['llm_explanation']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*30)\n",
    "\n",
    "# Run tests\n",
    "test_pipeline()\n",
    "\n",
    "# =================================\n",
    "# 6. Success criteria validation\n",
    "# =================================\n",
    "def validate_success_criteria():\n",
    "    \"\"\"Validate that all success criteria are met\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS CRITERIA VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    criteria = {\n",
    "        \"✓ Classifier trained and saved\": os.path.exists(\"../models/logreg.pkl\") and os.path.exists(\"../models/xgb.pkl\"),\n",
    "        \"✓ FAISS index built\": index.ntotal > 0,\n",
    "        \"✓ RAG retrieval working\": len(retrieve_relevant_docs(\"diabetes test\", k=3)) == 3,\n",
    "        \"✓ Pipeline runs end-to-end\": True,  # Will be tested below\n",
    "        \"✓ JSON output with required fields\": True  # Will be validated below\n",
    "    }\n",
    "    \n",
    "    # Test pipeline execution\n",
    "    try:\n",
    "        sample_patient = X_test.iloc[0].tolist()\n",
    "        result = enhanced_rag_pipeline(sample_patient)\n",
    "        \n",
    "        # Check required output structure\n",
    "        required_fields = ['prediction', 'probability', 'retrieved_documents', 'llm_explanation']\n",
    "        has_required_fields = all(field in result for field in required_fields)\n",
    "        criteria[\"✓ Pipeline runs end-to-end\"] = 'error' not in result\n",
    "        criteria[\"✓ JSON output with required fields\"] = has_required_fields\n",
    "        \n",
    "    except Exception as e:\n",
    "        criteria[\"✓ Pipeline runs end-to-end\"] = False\n",
    "        criteria[\"✓ JSON output with required fields\"] = False\n",
    "        print(f\"Pipeline test failed: {e}\")\n",
    "    \n",
    "    # Print results\n",
    "    for criterion, passed in criteria.items():\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        print(f\"{criterion}: {status}\")\n",
    "    \n",
    "    all_passed = all(criteria.values())\n",
    "    print(f\"\\nOVERALL STATUS: {'SUCCESS' if all_passed else 'NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# Validate success criteria\n",
    "validate_success_criteria()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PHASE 2 PROTOTYPE COMPLETE\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
