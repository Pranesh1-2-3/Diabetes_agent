{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23ca57ba",
   "metadata": {},
   "source": [
    "# Phase 5: Build RAG Store & Retrieval Service\n",
    "\n",
    "This notebook implements a RAG (Retrieval-Augmented Generation) system for medical guidelines with:\n",
    "1. PDF text extraction and chunking\n",
    "2. Embedding computation\n",
    "3. FAISS vector indexing\n",
    "4. Search functionality testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d4750",
   "metadata": {},
   "source": [
    "## 1. Setup Dependencies and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5157b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prane\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'docs_dir': '../data/docs',\n",
    "    'index_dir': '../data/index',\n",
    "    'chunk_size': 300,  # target number of tokens per chunk\n",
    "    'chunk_overlap': 50,  # number of tokens to overlap between chunks\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',  # sentence-transformers model\n",
    "    'index_file': 'faiss_index.bin',\n",
    "    'metadata_file': 'chunks_metadata.json',\n",
    "    'text_extraction': {\n",
    "        'strategy': 'blocks',  # Options: 'text', 'blocks', 'dict'\n",
    "        'combine_paragraphs': True,  # Combine text blocks into paragraphs\n",
    "        'min_block_size': 20  # Minimum characters in a text block\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create index directory if it doesn't exist\n",
    "os.makedirs(Path(CONFIG['index_dir']), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df58a02c",
   "metadata": {},
   "source": [
    "## 2. Text Extraction and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e768e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing IDF_Atlas.pdf...\n",
      "Extracted 6 pages and created 6 chunks\n",
      "\n",
      "Processing WHO_Fact_Sheet.pdf...\n",
      "Extracted 8 pages and created 8 chunks\n",
      "\n",
      "Total chunks created across all documents: 14\n",
      "\n",
      "Sample chunk:\n",
      "Document: IDF_Atlas, Page: 1\n",
      "Text: FAQs Contact Data by location Data by indicators Resources 589 million adults (20-79 years) are living with diabetes worldwide Explore diabetes around the world Download 2025 Report Donate We value your privacy We use cookies to enhance your browsing experience, serve personalized ads or content, an...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean extracted text by normalizing whitespace and removing artifacts.\"\"\"\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove any non-printable characters\n",
    "    text = ''.join(char for char in text if char.isprintable())\n",
    "    \n",
    "    # Normalize quotes and dashes\n",
    "    text = text.replace('\"', '\"').replace('\"', '\"').replace('—', '-')\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract text from PDF using PyMuPDF with block-based extraction.\"\"\"\n",
    "    pages = []\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    try:\n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            blocks = []\n",
    "            \n",
    "            if CONFIG['text_extraction']['strategy'] == 'blocks':\n",
    "                # Get text blocks with their bounding boxes and other properties\n",
    "                page_dict = page.get_text(\"dict\")\n",
    "                \n",
    "                # Extract and process text blocks\n",
    "                for block in page_dict.get('blocks', []):\n",
    "                    if block.get('type') == 0:  # Text blocks\n",
    "                        block_text = ''\n",
    "                        for line in block.get('lines', []):\n",
    "                            for span in line.get('spans', []):\n",
    "                                block_text += span.get('text', '') + ' '\n",
    "                        \n",
    "                        if len(block_text.strip()) >= CONFIG['text_extraction']['min_block_size']:\n",
    "                            blocks.append(clean_text(block_text))\n",
    "                \n",
    "            else:\n",
    "                # Fallback to simple text extraction\n",
    "                text = page.get_text()\n",
    "                if text.strip():\n",
    "                    blocks = [clean_text(text)]\n",
    "            \n",
    "            # Combine blocks if needed\n",
    "            if CONFIG['text_extraction']['combine_paragraphs']:\n",
    "                text = ' '.join(blocks)\n",
    "            else:\n",
    "                text = '\\n'.join(blocks)\n",
    "            \n",
    "            if text.strip():\n",
    "                pages.append({\n",
    "                    'page_num': page_num,\n",
    "                    'text': text,\n",
    "                    'doc_id': Path(pdf_path).stem\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {pdf_path}: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        doc.close()\n",
    "        \n",
    "    return pages\n",
    "\n",
    "def create_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "    \"\"\"Split text into overlapping chunks by word count, falling back if sentence splitting fails.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = ' '.join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # move with overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    " \n",
    "# Process all PDFs in docs directory\n",
    "chunks_with_metadata = []\n",
    "docs_dir = Path(CONFIG['docs_dir'])\n",
    "\n",
    "for pdf_file in docs_dir.glob('*.pdf'):\n",
    "    print(f\"\\nProcessing {pdf_file.name}...\")\n",
    "    pages = extract_text_from_pdf(str(pdf_file))\n",
    "    \n",
    "    total_chunks = 0\n",
    "    for page in pages:\n",
    "        chunks = create_chunks(\n",
    "            page['text'], \n",
    "            CONFIG['chunk_size'], \n",
    "            CONFIG['chunk_overlap']\n",
    "        )\n",
    "        \n",
    "        for chunk_idx, chunk_text in enumerate(chunks):\n",
    "            chunks_with_metadata.append({\n",
    "                'doc_id': page['doc_id'],\n",
    "                'page_num': page['page_num'],\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'text': chunk_text\n",
    "            })\n",
    "            total_chunks += 1\n",
    "    \n",
    "    print(f\"Extracted {len(pages)} pages and created {total_chunks} chunks\")\n",
    "\n",
    "print(f\"\\nTotal chunks created across all documents: {len(chunks_with_metadata)}\")\n",
    "\n",
    "# Display a sample chunk to verify content quality\n",
    "if chunks_with_metadata:\n",
    "    print(\"\\nSample chunk:\")\n",
    "    sample = chunks_with_metadata[0]\n",
    "    print(f\"Document: {sample['doc_id']}, Page: {sample['page_num']}\")\n",
    "    print(f\"Text: {sample['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f2c2e2",
   "metadata": {},
   "source": [
    "## 3. Create Embedding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3163eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "model = SentenceTransformer(CONFIG['embedding_model'])\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "texts = [chunk['text'] for chunk in chunks_with_metadata]\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Convert to numpy array for FAISS\n",
    "embeddings_np = np.array(embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb43104",
   "metadata": {},
   "source": [
    "## 4. Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3addc723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved to ..\\data\\index\\faiss_index.bin\n",
      "Metadata saved to ..\\data\\index\\chunks_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize FAISS index\n",
    "dimension = embeddings_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to the index\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Save the index\n",
    "index_path = Path(CONFIG['index_dir']) / CONFIG['index_file']\n",
    "faiss.write_index(index, str(index_path))\n",
    "\n",
    "# Save chunk metadata\n",
    "metadata_path = Path(CONFIG['index_dir']) / CONFIG['metadata_file']\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(chunks_with_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Index saved to {index_path}\")\n",
    "print(f\"Metadata saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71e71e",
   "metadata": {},
   "source": [
    "## 5. Implement Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc2ae463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_chunks(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Search for relevant chunks given a query.\"\"\"\n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), k)\n",
    "    \n",
    "    # Get results with metadata\n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        chunk_data = chunks_with_metadata[idx].copy()\n",
    "        chunk_data['distance'] = float(distance)\n",
    "        results.append(chunk_data)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example search function for the FastAPI endpoint\n",
    "def create_search_response(query: str, k: int = 3) -> Dict[str, Any]:\n",
    "    results = search_chunks(query, k)\n",
    "    return {\n",
    "        'query': query,\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937e5aa",
   "metadata": {},
   "source": [
    "## 6. Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe4d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running search quality tests...\n",
      "\n",
      "Query: global prevalence of diabetes\n",
      "Found expected content: ✓\n",
      "Top result: The number of people living with diabetes rose from 200 million in 1990 to 830 million in 2022. Prevalence has been rising more rapidly in low- and middle-income countries than in high-income countrie...\n",
      "\n",
      "Query: symptoms of diabetes\n",
      "Found expected content: ✓\n",
      "Top result: Symptoms of diabetes may occur suddenly. In type 2 diabetes, the symptoms can be mild and may take many years to be noticed. Symptoms of diabetes include: feeling very thirsty needing to urinate more ...\n",
      "\n",
      "Query: type 1 diabetes characteristics\n",
      "Found expected content: ✓\n",
      "Top result: Type 1 diabetes (previously known as insulin-dependent, juvenile or childhood- onset) is characterized by deficient insulin production and requires daily administration of insulin. In 2017 there were ...\n",
      "\n",
      "Query: causes of type 2 diabetes\n",
      "Found expected content: ✓\n",
      "Top result: Type 1 diabetes (previously known as insulin-dependent, juvenile or childhood- onset) is characterized by deficient insulin production and requires daily administration of insulin. In 2017 there were ...\n",
      "\n",
      "Query: gestational diabetes risks\n",
      "Found expected content: ✓\n",
      "Top result: complications have already arisen. More than 95% of people with diabetes have type 2 diabetes. Type 2 diabetes was formerly called non-insulin dependent, or adult onset. Until recently, this type of d...\n",
      "\n",
      "Query: prevention of type 2 diabetes\n",
      "Found expected content: ✓\n",
      "Top result: Lifestyle changes are the best way to prevent or delay the onset of type 2 diabetes. To help prevent type 2 diabetes and its complications, people should: reach and keep a health body weight stay phys...\n",
      "\n",
      "Test Results: 6/6 queries successful\n",
      "Retrieval Quality: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Test queries with expected content\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'global prevalence of diabetes',\n",
    "        'expected_keywords': ['830 million', '2022', 'prevalence', 'low- and middle-income']\n",
    "    },\n",
    "    {\n",
    "        'query': 'symptoms of diabetes',\n",
    "        'expected_keywords': ['thirsty', 'urinate', 'blurred vision', 'tired', 'losing weight']\n",
    "    },\n",
    "    {\n",
    "        'query': 'type 1 diabetes characteristics',\n",
    "        'expected_keywords': ['insulin', 'deficient', 'daily', 'juvenile']\n",
    "    },\n",
    "    {\n",
    "        'query': 'causes of type 2 diabetes',\n",
    "        'expected_keywords': ['overweight', 'exercise', 'genetics', 'preventable']\n",
    "    },\n",
    "    {\n",
    "        'query': 'gestational diabetes risks',\n",
    "        'expected_keywords': ['pregnancy', 'delivery', 'complications', 'type 2']\n",
    "    },\n",
    "    {\n",
    "        'query': 'prevention of type 2 diabetes',\n",
    "        'expected_keywords': ['healthy diet', 'exercise', 'weight', 'tobacco']\n",
    "    }]\n",
    "\n",
    "def evaluate_search_results(results: List[Dict[str, Any]], \n",
    "                          expected_keywords: List[str]) -> bool:\n",
    "    \"\"\"Check if any result contains all expected keywords.\"\"\"\n",
    "    for result in results:\n",
    "        text = result['text'].lower()\n",
    "        if all(keyword.lower() in text for keyword in expected_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Run tests\n",
    "print(\"Running search quality tests...\\n\")\n",
    "successes = 0\n",
    "\n",
    "for test in test_queries:\n",
    "    print(f\"Query: {test['query']}\")\n",
    "    results = search_chunks(test['query'])\n",
    "    \n",
    "    success = evaluate_search_results(results, test['expected_keywords'])\n",
    "    successes += int(success)\n",
    "    \n",
    "    print(f\"Found expected content: {'✓' if success else '❌'}\")\n",
    "    print(f\"Top result: {results[0]['text'][:200]}...\\n\")\n",
    "\n",
    "print(f\"Test Results: {successes}/{len(test_queries)} queries successful\")\n",
    "print(f\"Retrieval Quality: {(successes/len(test_queries))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675907a-1fc5-406a-ac66-2a37c82011a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
